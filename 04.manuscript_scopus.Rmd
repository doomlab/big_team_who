---
title             : "Who does big team science?"
shorttitle        : "Big Team Science"
author: 
  - name          : "Erin M. Buchanan"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "326 Market St., Harrisburg, PA 17101"
    email         : "ebuchanan@harrisburgu.edu"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - Conceptualization
      - Data curation
      - Formal Analysis
      - Methodology
      - Project administration
      - Visualization
      - Writing – original draft
      - Writing – review & editing
  - name          : "Savannah C. Lewis"
    affiliation   : "2"
    role:
      - Conceptualization
      - Data curation
      - Methodology
      - Project administration
      - Writing – original draft
      - Writing – review & editing
affiliation:
    
  - id            : "1"
    institution   : "Harrisburg University of Science and Technology"
  - id            : "2"
    institution   : "University of Alabama"
authornote: |
  Erin M. Buchanan is a Professor of Cognitive Analytics at Harrisburg University of Science and Technology. Savannah C. Lewis is a graduate student at the University of Alabama.  
  
  Thank you to Dwayne Lieck for providing an extensive list of large scale projects for this manuscript. 
abstract: |
  This paper will examine the nature of publications in Big Team Science (BTS) - large-scale collaborations between multiple researchers at multiple institutions. As interest in BTS increases, it is useful to explore who is currently involved in BTS projects to determine diversity in both research subject and researcher representation. The types of publication outlets, number of publications, and subject areas of publication will be presented to summarize the publications in BTS. Information about authors included in BTS will be presented including career length, numbers of publications/impact variables, education, and affiliation. Last, we will explore the representation of geopolitical regions by examining affiliation location to explore the impact of BTS on the de-WEIRD movement to diversify researcher representation. 
  
keywords          : "big team, science, authorship, credit"
wordcount         : "X"
floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no
#documentclass     : "apa6"
classoption       : "man"
#output            : papaja::apa6_pdf
editor_options: 
  markdown: 
    wrap: 72
bibliography: references.bib

header-includes:
  - |
    \makeatletter
    \renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-1em}%
      {\normalfont\normalsize\bfseries\typesectitle}}
    
    \renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-\z@\relax}%
      {\normalfont\normalsize\bfseries\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
    \makeatother

csl               : "`r system.file('rmd', 'apa7.csl', package = 'papaja')`"
documentclass     : "apa7"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library(papaja)
library(knitr)
```

# Significance

According to the Oxford English dictionary, collaboration is two or more
people working together to achieve a certain goal [@oed2016].
Collaboration in scientific endeavors involves multiple researchers at
(potentially) multiple institutions to communicate and work together to
advance knowledge in their chosen field. Collaboration can manifest
uniquely in each project dependent on the skill sets, hypotheses, and
perspectives of collaborators. While collaboration is not new in
science, the current interest of "big team science" is increasing
[@forscher2020; @stewart2017; @coles2022]. Big team science projects
and/or organizations utilize and run on large-scale collaboration to
ensure that diverse populations and ideas are brought into research
projects, which in turn allows for more reliability and generalizability
in the results and method of the study. For this study, Big Team Science
(BTS) will be defined as a collaboration of ten or more authors from at
least ten different institutions.

BTS appears to be increasing as a result of two sources: 1) increasing
globalization and technology that allows for real-time interdisciplinary
research, and 2) increasing interest in reproducibility, replication,
and generalizability [@maxwell2015; @nelson2018; @zwaan2018].
Technological advances have provided easier ways to collaborate with
people who are from other universities and countries through document
sharing platforms (e.g., Google, GitHub, and the Open Science
Framework), video chatting platforms (e.g., Zoom, Microsoft Teams), and
messaging and project management platforms (e.g., Slack, Trello,
when2meet, etc.). The credibility movement seems to suggest that by
having both collaborations that span across the globe and subfield of
psychology, age groups, and education levels should help to drive
psychological science in the path of better materials, reliability,
generalizability and more robust sample size in a study
[@auspurg2021has; @nosek2014method; @lebel2018].

The credibility movement was originally defined by a focus on large
scale replications using in collaborative environments [@vazire2022].
Generally, the movement has been driven by early career researchers
(i.e., those who are within five years of their first appointment)
[@maizey2019]; however, there are no large meta-scientific
investigations on this specific topic to date. Potentially, the lack of
investigation is tied to the newness of the large-scale research in many
fields, as it is only in recent years that publications like the Open
Science Collaboration [@opensciencecollaboration2015], Many Labs
Collaborations [for example, @klein2018; @ebersole2020; @ebersole2016;
@klein2022; @mathur2020; @buttrick2020; @skorb2020] or the first papers
from the Psychological Science Accelerator [@moshontz2018; @jones2021;
@dorison2022; @wang2021; @legate2022; @bago2022]. Generally, the
researcher incentive for replication was low: journals often prioritize
"novel" or new results which led to rejection of replication manuscripts
and publication bias [@hubbard1997; @franco2014; @nosek2012], the
"failure" to replicate was often placed on the replication team as "bad
science" rather than a careful consideration of publication biases and
(potential) questionable research practices [@ioannidis2015;
@maxwell2015; @klein2022], and why should someone want to spend time and
resources on an answer we already "know" [@isager2021; @isager2021a]?

However, the success and interest in the large-scale reproducibility
projects [@opensciencecollaboration2015; @10.7554/eLife.71601], paired
with the meta-scienctific publications focusing on researcher practices
and incentive structures [@silberzahn2018many; @john2012] led to a
change in journal guidelines and incentives for researchers interested
in participating in large-scale replication studies [@grahe2014;
@kidwell2016; @nosek2015; @mayo-wilson2021]. For example, the support
for Registered Reports, papers accepted before the data has been
collected [@nosek2014; @stewart2020], and entire sub-sections of
journals devoted to only replication studies (e.g., *Nature, Royal
Society Open Science, Advances in Methods and Practices in Psychological
Science*) has allowed researchers to invest in projects that they know
should be published when the project is complete. Further, the
implementation of the Transparency and Openness Guidelines [@nosek2015]
and the Contributor Role Taxonomy (CRediT) system [@allen2019] have
pushed journals and researchers to promote more open, inclusive
publication practices.

The credibility movement has been mirrored by the calls for
diversification or de-WEIRDing (e.g., Western, Educated, Industrialized,
Rich, and Democratic) scientific research [@henrich2010; @newson2021;
@rad2018] by improving representation in research samples. Like the
large-scale studies in Physics [@aphilos2021; @castelnovo2018] and
Biology [@collins2003], the social sciences struggle to represent the
breadth of humanity across both researcher and population
characteristics. Now, grassroots organizations, such as the
Psychological Science Accelerator [@moshontz2018], ManyBabies
(<https://manybabies.github.io/>), NutNet (<https://nutnet.org/>), and
DRAGNet (<https://dragnetglobal.weebly.com/>) can begin to tackle these
issues by recruiting research labs from all over the globe to provide
diversity in geographic, linguistic, and researcher representation.
Publications have examined the global understanding of morality, face
processing, COVID-19 information signaling, and more [@dorison2022;
@wang2021; @legate2022; @bago2022; @jones2021; @vanbavel2022]. While
these organizations and one-time groups for BTS studies have provided an
incredible wealth of data for the scientific community, we do not yet
know exactly *who* is involved with, and benefits from, the BTS and
credibility movement. Publications on BTS generally explore challenges,
lessons learned, the need for BTS [@forscher2020; @coles2022].

Therefore, the goal of this manuscript is to examine the *people*
involved in BTS projects. We specifically expect to examine ICSR's
Research Themes of inclusivity, research careers, and research
globalization. As we examine these themes, it will bring new knowledge
of how BTS projects impact each theme and field of study. We see an
increase in interest and publications in BTS but we do not yet know if
this uptick in large-scale projects has diversified the *people*
involved in BTS. While a few publications have noted that BTS appears to
be early career researchers [@maizey2019], no one has systematically
investigated this perception. Further, it is unclear if the focus of
de-WEIRDing science has only focused on the representation of the
research participants or if it has also improved the representation of
researchers outside of North America and Europe. Last, who runs these
BTS projects? Do we see an increase in diversity for the authors who
generally receive the most credit for these projects (i.e., first
several author(s) and last author)? As hiring and promoting practices
often place a heavy weight on publications and especially "influential"
publications, it becomes necessary to critically examine the
representation present in authorship in BTS projects.

# Potential Outlets

We will aim for high impact broad scope journals such as *Science*,
*Nature* or *Nature Human Behaviour*. Other journals would include
review publications within psychology to compare the social sciences to
other sciences: *Perspectives in Psychological Science*, *Psychological
Bulletin*, *Psychological Review*, or *Current Directions in
Psychological Science*.

# Approach

-   Research Question 1: What publication sources publish big team
    science papers?
-   Research Question 2: What are the types of articles that are being
    published in big team science?
-   Research Question 3: Who is involved in big team science?

For each of these research questions, we will examine the overall
results of all big team research projects, and examine for change in
result trends across years of publication. Below we detail our methods
and the ICSR/Scopus data to answer these questions, along with examples
of the statistical results we expect to report in the manuscript. We
began this project with data using Google Scholar and ORCID information.
These sources were severely limited in their scope and breadth, as they
are often curated with automatic processes or self-entered data, and we
believe that access to Scopus and ICSR would allow us to accurately
portray the BTS movement and its impact on diversity across many fields.
The novelty of this project is that it would focus on all of published
works, rather than a specific subfield (like Psychology) and give a lens
into global representation in science that would otherwise not be
achieved with open-source databases.

# Method

## Publications

We have defined **BTS publications** as publications with at least 10
authors at 10 different institutions that were published in
peer-reviewed journals or had posted a full paper pre-print. We will use
data from 1970 and forward in the publications (`ani)` database, as it
is noted online that this time period includes cited references for
calculation of several of our variables described below. We will analyze
our results based on the big four subject areas: Physical Sciences,
Health Sciences, Social Sciences, and Life Sciences.

## Data Curation

### RQ1: Publisher Information.

Using these criteria, we will extract the following information for
publication sources: the name of the publication (source title), subject
area (both the large four subject areas and the smaller four digit all
science journal classification ASJC codes). We will examine journal
impact using the Source Normalized Impact per Paper from the journal
(`sources)` database.

### RQ2: Publication Information.

For each publication of the identified BTS publications, we will analyze
the full four digit ASJC subject areas codes for each of the larger four
subject areas and the keywords present for these publications.

### RQ3: Author Information.

The author list will then be extracted from each publication. Next, we
will use the author (`au`) and affiliation (`af`) array to curate a list
of all publications and author information included in BTS papers. We
will use these two arrays with the publication array to calculate the
variables described below.

***Career Length***. Career length for each author will be defined as
the year of the first publication listed for each author.

***Institution***. We will use the affiliation ids and country to gather
information about the places of education and/or employment for authors.
Country will likely be binned into United Nation Sub-Region for
analyses.

***Education***. We will also collect degree information from the author
table.

***Types of Publications***. We will gather information from the
publication type variable for each author publication to present
information about the types of papers BTS authors publish.

***Publication Metrics***. For each author, we will calculate the total
number of publications, the h-index, and the i-10 index. The h-index
represents the highest *h* number of publications that have at least *h*
citations, while the i-10 index represents the number of publications
with at least 10 citations.

## Data analysis

### RQ1: Publisher Information.

To present results on this research question, we will analyze:

-   Number of articles for inclusion: total, separated by four subject
    areas, presenting graphics of the number of publication across time
-   Number of distinct journals within each of the four subject areas
-   Statistics (mean, median, standard deviation, minimum, maximum) of
    the journal impact using Source Normalized Impact per Paper for the
    four subject areas.

### RQ2: Publication Information.

For each publication, we will examine:

-   The totals of the number of articles published within the smaller
    subject area classifications. We will visualize these differences
    over time to determine if there is increasing interest in a specific
    subarea over time for each of the four larger subject area
    classifications.
-   The keywords present in the publications data overall to identify
    trends and common themes in the publications for the four subject
    areas using visualizations (wordclouds) to depict the common
    keywords.

### RQ3: Authors.

We will first present:

-   The total number of unique authors
-   Statistics (mean, standard deviation, minimum, maximum, median) on
    the number of authors included on publications.
-   We will present visualizations of these results across time.

We will use $\alpha$ \< .05 for all analyses that involve hypothesis
testing. We make no directional predictions.

***Career Length***.

-   We will create a visualization of the trend and variance of
    researcher career length across publication years.
-   To analyze trends over time, we will calculate the average career
    length for each publication (i.e., average the author career length
    to create one score for each paper) and run a regression analysis
    using career length to predict year of publication. A positive slope
    for year of publication would indicate increasing years of first
    publication (i.e., more younger scholars over time), while a
    negative slope would indicate older years of first publication
    (i.e., more older scholars over time).
-   In order to show variance between individuals, we calculate the
    standard deviation of career length for each publication and run a
    regression analysis using this variance representation to predict
    publication year. A positive slope would indicate increasing
    variance over time (i.e., more diversity in the career lengths of
    scholars), while a negative slope would indicate less variance and
    diversity in scholars over time.
-   These analyses will be completed separately for each of the four
    large subject areas.

***Institution***.

-   We will summarize the number of affiliation ids present in BTS
    publications by subject area and visualize these results across
    time. These visualizations will be presented separately for each of
    the four subject areas.

***Education***.

-   We will summarize the general education categories of individuals at
    the time of publication, along with a summary for change over time.

***Types of Publications***.

-   We will summarize the coded types of publications for individuals.

***Publication Metrics***.

-   We will report descriptive statistics on the total number of
    publications, i10 index, and h-index for individuals overall.
-   Next, we will use the same analyses described in the career length
    section to analyze trends over time. An increasing slope over time
    indicates that individuals who are publishing more are more
    represented in BTS over time (i.e., increasing numbers of scholars
    with higher publication rates), while a negative slope indicates
    more researchers with less publications.
-   A positive slope for standard deviation indicates increasing
    variance over time (i.e., more diversity in the individual
    publication rates), while a negative slope would indicate less
    diversity in researchers over time. While publication rates do not
    represent value as a researcher, they are often used in hiring and
    promotion decisions, and we will use this variable as a proxy to
    gauge the diversity in scholars represented in big teams.

***Geopolitical Regions***.

-   We will present visualizations of the country information listed for
    authors, and we will discuss the areas of world in which authors
    generally come from, as well as the lowest representation of
    authors.
-   To understand the change in representation diversity, we will
    summarize the total number of geopolitical regions for each paper.
    Using a linear model, we will examine if the number of regions
    present is predicted by the year of publication. Increasing
    diversity would be represented by a positive slope, while decreasing
    diversity would be represented by a negative slope.
-   Last, we will examine the differences in representation for
    corresponding author sets versus all other authors. For papers with
    10 to 49 authors, we will use the three first authors and the last
    author to compare against other authors. For 50 to 99 authors, five
    first authors plus last will be used, and for all papers with more
    than 100 authors, we will use ten first authors and the last author.
    We will calculate the frequencies of each of the UN Sub-Regions for
    first authors versus other authors, converting these values to
    proportions. Given the expected small sample sizes of these
    contingency tables, we will group together titles based on the year
    of publication (assuming at least 5 publications per year, these may
    be binned by 5-year or smaller increments to increase sample size).
    For each grouping, we will calculate the effect size of the
    differences in frequencies comparing first authors to all other
    authors. Since this data is categorical, we will use Cramer's V to
    represent the effect size. If the effect size includes zero in its
    confidence interval, this result will imply that first and all other
    authors represent the same pattern of UN Sub-Region diversity. Any
    confidence interval that does include zero represents a difference
    in diversity. We will report these values and discuss what regions
    of the world are represented when effect sizes indicate a different
    from zero using standardized residuals.

\newpage

# References

```{=tex}
\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
```
::: {#refs custom-style="Bibliography"}
:::

```{=tex}
\endgroup
```
